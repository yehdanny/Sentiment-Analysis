{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e0X3sSm84C1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8sYpApn86QS"
      },
      "outputs": [],
      "source": [
        "#print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PrDx_PMzYJss"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentencepiece accelerate\n",
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd3rOK0rYPd6"
      },
      "source": [
        "## 載入資料集與模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SK_9hBzQYMvc"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# 載入資料集\n",
        "dataset = load_dataset(\"Maciel/FinCUGE-Instruction\")\n",
        "\n",
        "# 載入預訓練的中文 BERT 模型與 tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRq9IfWV7RVj"
      },
      "source": [
        "用 Dataset.filter() 清除無效樣本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsmWkIaMKkdn"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "dataset_org = dataset\n",
        "# 使用 论坛情绪分析任务\n",
        "filtered_dataset = dataset_org.filter(lambda example: example['desc'] == '论坛情绪分析任务')\n",
        "dataset = filtered_dataset\n",
        "# 顯示筆數\n",
        "print(f\"Train before :{len(dataset_org['train'])}, after :{len(dataset['train'])}\")\n",
        "print(f\"Eval before :{len(dataset_org['eval'])}, after :{len(dataset['eval'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bx53K9D5kWK"
      },
      "source": [
        "從自然語言中抽出情緒詞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIRg2iNW4iGj"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "label_map = {\n",
        "    \"积极\": 0,\n",
        "    \"消极\": 1,\n",
        "    \"中性\": 2\n",
        "}\n",
        "def extract_label(output_str):\n",
        "    # 簡單的方式：直接尋找關鍵詞\n",
        "    for key in label_map:\n",
        "        if key in output_str:\n",
        "            return label_map[key]\n",
        "    # 找不到就視為無效\n",
        "    raise ValueError(f\"無法從 output 中解析出標籤: {output_str}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX73wAkD0Hsd"
      },
      "source": [
        "資料預處理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jjV4_P_lvI4Z"
      },
      "outputs": [],
      "source": [
        "'''#acc : 0.78\n",
        "def preprocess_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples['input'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=128\n",
        "    )\n",
        "    #抓出情緒詞\n",
        "    tokenized[\"labels\"] = [extract_label(o) for o in examples[\"output\"]]\n",
        "    return tokenized\n",
        "'''\n",
        "def preprocess_function(examples):\n",
        "    texts = [i + \" \" + j for i, j in zip(examples[\"instruction\"], examples[\"input\"])]\n",
        "    tokenized = tokenizer(texts, truncation=True, padding='max_length', max_length=128)\n",
        "    tokenized[\"labels\"] = [extract_label(o) for o in examples[\"output\"]]\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fEBcJE8u7LC"
      },
      "source": [
        "訓練模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AB2hCr5uYOFC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from transformers import EarlyStoppingCallback\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    learning_rate=1e-5,  #LR\n",
        "    warmup_steps=500,   #LR\n",
        "    load_best_model_at_end=True,#Early Stop\n",
        "    save_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "#驗證指標\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(axis=-1)\n",
        "    return {\n",
        "        'accuracy': accuracy_score(labels, preds),\n",
        "        'f1': f1_score(labels, preds, average='macro')\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"eval\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] #EARLY STOP\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlOxqwCjbQBX"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "model_path = f\"./results/model_{timestamp}\"\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "trainer.save_model(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uzh4Kvko1TMh"
      },
      "outputs": [],
      "source": [
        "timestamp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFf_BUT61SAq"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# 載入 tokenizer 和 微調後的模型\n",
        "model_path = f\"./results/model_{timestamp}\"  # 假設你存在這個目錄\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "model.eval()  # 設定為推論模式\n",
        "\n",
        "# 定義標籤對應（你訓練時應該是三類情感）\n",
        "label_map = {0: \"消極\", 1: \"中性\", 2: \"積極\"}\n",
        "\n",
        "text = \"這支股票今天開低走高，明天應該會繼續漲！\"\n",
        "\n",
        "# 前處理\n",
        "input_text = \"这个文本的情感倾向是积极、消极还是中性的。 \" + text  # 與訓練時格式一致\n",
        "# Tokenize 輸入\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "\n",
        "\n",
        "\n",
        "# 預測\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predicted_class_id = torch.argmax(logits, dim=1).item()\n",
        "    predicted_label = label_map[predicted_class_id]\n",
        "\n",
        "# 輸出結果\n",
        "print(f\"輸入句子：{text}\")\n",
        "print(f\"預測情感：{predicted_label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u36aTixintAW"
      },
      "outputs": [],
      "source": [
        "#rm -r results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX2EbaXX2_N6"
      },
      "source": [
        "類別平衡?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJPozVBAnupD"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "Counter(tokenized_datasets[\"train\"][\"labels\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjlzDgx_3AwZ"
      },
      "source": [
        "CONFUSION MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xi2xJqD71raX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "preds = trainer.predict(tokenized_datasets[\"eval\"]).predictions.argmax(axis=1)\n",
        "labels = tokenized_datasets[\"eval\"][\"labels\"]\n",
        "print(confusion_matrix(labels, preds))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}